{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Should you use synthetic data for label balancing\"\n",
    "\n",
    "After having worked with tabular synthetic data for the past 6 months, I have encountered many articles claiming that synthetic data is the solution for many (machine learning) problems. While synthetic data can serve as a useful Privacy Enhancing Technology (PET) and has shown to be useful in certain tasks, its usefulness and relevance is not always clearly assessed. An example of this is, and also the inspiration for me writing this article, is the article provided by Synthetic Data Vault (SDV) titled: \"Can you use synthetic data for label balancing?\" (https://sdv.dev/blog/synthetic-label-balancing/).\n",
    "\n",
    "The article addresses a well-known issue in classification problems: imbalanced target labels. The article correctly identifies techniques like Random Oversampling (ROS) and noise injection while acknowledging their downsides (being overfitting and noisy data). However, the article then presents synthetic data as a \"compelling solution\" without providing enough evidence of this. While I am a fan of SDV, their generators, preprocessors, and constraints, this article overlooks important aspects validating the validity of synthetic data for these problems. Although you definitely can use synthetic data for label balancing (to answer the question of the article), the key question is whether you **should** use synthetic data and how it compares to state-of-the-art (SOTA) techniques.\n",
    "\n",
    "Throughout this article, I aim to provide an answer to this question by comparing synthetic data produced by SDV generators against alternatives and build on top of the aformentioned article. Specifically, I compare data-level approaches such as noise injection, ROS, Synthetic Minority Over-sampling TEchnique (SMOTE), and CTGAN against the algorithm-level approach of Cost-Sensitive learning. This idea is not novel and adjacent research is available in literature. Adiputra and Wanchai (2024) for instance compare similar approaches resampling and synthetic data approaches. However, their calidation approach uses cross validation while resampling before cross validation, which is a common pitfall leading to data leakage.\n",
    "\n",
    "This article aims to improve on this by providing a more methodologically sound approach whilst providing the intuition and explanation for practictioners that are less familiar with imbalanced classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Check the helper_functions\n",
    "    - move _get_num_samples to init?\n",
    "    - Fix scoring function for results\n",
    "    - Should I use fit/transform as opposed to fit_resample/resample in the custom samplers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Notably, to ensure a proper cross validation procedure, I use a pipeline. Specifically, the pipeline from imb_learn is used over sklearn's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt     \n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import CTGANSynthesizer, TVAESynthesizer\n",
    "\n",
    "from helper_functions import NoiseSampler, ColumnScaler, SDVSampler, get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "For this analysis, the creditcard dataset will be used from Kaggle (https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud), containing transactions and whether they were fraudulent or not. The goal is to predict whether a transaction is fraudulent or not, making it a classification task. Naturally, the amount of non-fraudulent transactions outweigh the number of fraudulent transactions resulting in an imbalanced classification task.\n",
    "\n",
    "For this analysis, we will only be using a subset of the columns, which in this case contains mostly the Principal Components of the original data for confidentiality reasons. In addition, we also have a variable called \"Time\" which is the seconds from the first transaction in the dataset, 'Amount' which is the amount spent on the transaction, and 'Class' indicating whether the transactions is fraudulent or not, which also is our target variable. Ultimately, the data consists mostly of floats with the exception of our target which is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Select first then and last 2 columns\n",
    "creditcard = pd.read_csv('../data/creditcard.csv')\n",
    "\n",
    "# To reduce the dimensiolality of the dataset, we will only use the first 12 principal components,\n",
    "# Time, Amount, and target\n",
    "print(creditcard.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we see how our target labels are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    99.83\n",
       "1     0.17\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target label distribution rounded to 2 decimal places\n",
    "round(creditcard['Class'].value_counts(normalize = True) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes evident that the dataset is highly imbalanced, with  only 0.17% of transactions being fraudulent. This can be problematic for machine learning algorithms if you were to not account for this as models tend to bias towards the non-fraudulent cases given their overrepresentation in the data. As a result, a model predicting every transaction to be non-fraudulent would already result in a 99% accuracy even though every fraudulent transaction has been wrongfully predicted. This highlights the importance of carefully selecting evaluation metrics, as accuracy alone can be misleading in imbalanced classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "Next step is to split the data. We stratify on the target variable to ensure an even split across train/set sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = creditcard.drop('Class', axis = 1)\n",
    "y = creditcard['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "As mentioned previously, the choice of metrics is far from trivial. However, given that it is beyond the scope of this blog, I will choose an F1-score without diving too deep into the costs of misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "For the algorithm, I will use a LGBM Classifier. Choosing the most optimal estimator is beyond the scope of this blog. LGBM is chosen for its efficiency and relative predictive power, therefore being used consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "We define the folds and parameters to optimize over as these will be consistent across resampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2\n",
    "\n",
    "CV_FOLDS = StratifiedKFold(n_splits = 5, random_state = RANDOM_STATE, shuffle = True)\n",
    "\n",
    "scorings = ['f1', 'roc_auc', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance\n",
    "\n",
    "As a baseline for this problem to compare predictions to, we will use the DummyClassifier from sklearn. We use 'stratified' becasue choosing 'most_frequent' is probably also a natural choice, but given that we mostly evaluate on F1, all scores would be zero, making the improvement over the baseline seem better more impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.08679271, 0.08679271, 0.08679271, 0.08679271, 0.08679271]),\n",
       " 'score_time': array([0.03021216, 0.03021216, 0.02011299, 0.03021216, 0.02011299]),\n",
       " 'test_f1': array([0., 0., 0., 0., 0.]),\n",
       " 'test_roc_auc': array([0.49912069, 0.49912069, 0.49912069, 0.49912069, 0.49912069]),\n",
       " 'test_precision': array([0., 0., 0., 0., 0.]),\n",
       " 'test_recall': array([0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_baseline = Pipeline(\n",
    "    [('dummy', DummyClassifier(strategy = 'stratified', random_state=RANDOM_STATE)),]\n",
    ")\n",
    "\n",
    "cv_score_base = cross_validate(pipeline_baseline, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                                n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise injection\n",
    "\n",
    "The first approach mentioned in the article is noise injection. Although I have not seen it being used in practice and the article does not mention the noise generating process, a uniform sampling procedure will be used. Specifically, for each variable I will extract their minimum and maximum values and use them to sample from a uniform distribution. Therefore, the correlations between variables are overlooked and the bivariate distributions won't be correct.\n",
    "\n",
    "Given that all variables, with exception of the target variable, are numerical this step is quite straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_noise = Pipeline(\n",
    "    [('noise_sampler', NoiseSampler(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_noise = cross_validate(pipeline_noise, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_ros = Pipeline(\n",
    "    [('ros', RandomOverSampler(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_ros = cross_validate(pipeline_ros, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_smote = Pipeline(\n",
    "    [('scaler', ColumnScaler(['Amount', 'Time'])),\n",
    "     ('smote', SMOTE(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_smote = cross_validate(pipeline_smote, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                                n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDV Generators\n",
    "\n",
    "The SDV generators that will be compared are the CTGAN and TVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = Metadata.detect_from_dataframe(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTGAN\n",
    "\n",
    "I will use the CTGAN synthesizer in this case with default parameters and without any constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_ctgan = Pipeline(\n",
    "    [('ctgan', SDVSampler(CTGANSynthesizer, metadata, random_state= RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_ctgan = cross_validate(pipeline_ctgan, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TVAE Synthesizer\n",
    "\n",
    "I will also use the TVAE Synthesizer. Again, there will be no hyperparameter tuning and no constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_tvae = Pipeline(\n",
    "    [('tvae', SDVSampler(TVAESynthesizer, metadata, random_state= RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_tvae = cross_validate(pipeline_tvae, X, y, cv=CV_FOLDS, scoring = scorings,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost sensitive learning\n",
    "\n",
    "I will be using the inverse class frequency for this as the real costs associated with misclassifying creditcard fraud is not known in this example. These weights will be specified using the classifier's 'class_weight' parameter. Specifically, this is set to 'balanced' to achieve the inversely proportional weights. \n",
    "\n",
    "Given that the folds are stratified, we assume the that the assigned weights for the entire train set is (roughly) equal to the weights for each fold. This will result in the following weights being assigned:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.50086524, 289.43800813])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0] / (2 * np.bincount(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline then becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_csl = Pipeline(\n",
    "    [('LGB', lgb.LGBMClassifier(class_weight= 'balanced', random_state = RANDOM_STATE,\n",
    "                                n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_csl = cross_validate(pipeline_csl, X, y, cv=CV_FOLDS, scoring = scorings,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8388369280184002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([4.70633245, 5.10437608, 4.82159638, 4.70733619, 4.71133566]),\n",
       " 'score_time': array([0.36897874, 0.15098023, 0.32333207, 0.38900161, 0.397228  ]),\n",
       " 'test_f1': array([0.86458333, 0.83333333, 0.83248731, 0.83838384, 0.82539683]),\n",
       " 'test_roc_auc': array([0.98446967, 0.97377153, 0.9660105 , 0.9700833 , 0.98033602]),\n",
       " 'test_precision': array([0.89247312, 0.80952381, 0.82828283, 0.83      , 0.85714286]),\n",
       " 'test_recall': array([0.83838384, 0.85858586, 0.83673469, 0.84693878, 0.79591837])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cv_score_csl['test_f1'].mean())\n",
    "cv_score_csl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f1', 'roc_auc', 'precision', 'recall']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4194102145.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[41], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "cv_scores = [cv_score_base, cv_score_noise, cv_score_ros, cv_score_smote,\n",
    "             cv_score_ctgan, cv_score_tvae, cv_score_csl]\n",
    "\n",
    "def display_scores(cv_scores, scorings):\n",
    "   res = {}\n",
    "   \n",
    "   for scoring in scorings:\n",
    "       res[scoring] = \n",
    "       \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the scores \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "y_preds = [y_pred_n, y_pred_ros, y_pred_smote, y_pred_ctgan, y_pred_csl]\n",
    "sampling_procedures = ['Noise', 'ROS', 'SMOTE', 'CTGAN', 'Cost-Sensitive Learning']\t\n",
    "\n",
    "get_scores(y_test, y_preds, sampling_procedures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imb_syn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
