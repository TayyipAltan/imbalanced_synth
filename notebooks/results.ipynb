{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Should you use synthetic data for label balancing\"\n",
    "\n",
    "After having worked with tabular synthetic data for the past 6 months, I have encountered many articles claiming that synthetic data is the solution for many (machine learning) problems. While synthetic data can serve as a useful Privacy Enhancing Technology (PET) and has shown to be useful in certain tasks, its usefulness and relevance is not always clearly assessed. An example of this is, and also the inspiration for me writing this article, is the article provided by Synthetic Data Vault (SDV) titled: \"Can you use synthetic data for label balancing?\" (https://sdv.dev/blog/synthetic-label-balancing/).\n",
    "\n",
    "The article addresses a well-known issue in classification problems: imbalanced target labels. The article correctly identifies techniques like Random Oversampling (ROS) and noise injection while acknowledging their downsides (being overfitting and noisy data). However, the article then presents synthetic data as a \"compelling solution\" without providing enough evidence of this. While I am a fan of SDV, their generators, preprocessors, and constraints, this article overlooks important aspects validating the validity of synthetic data for these problems. Although you definitely can use synthetic data for label balancing (to answer the question of the article), the key question is whether you **should** use synthetic data and how it compares to state-of-the-art (SOTA) techniques.\n",
    "\n",
    "Throughout this article, I aim to provide an answer to this question by comparing synthetic data produced by SDV generators against alternatives and build on top of the aformentioned article. Specifically, I compare data-level approaches such as noise injection, ROS, Synthetic Minority Over-sampling TEchnique (SMOTE), and CTGAN against the algorithm-level approach of Cost-Sensitive learning. This idea is not novel and adjacent research is available in literature. Adiputra and Wanchai (2024) for instance compare similar approaches resampling and synthetic data approaches. However, their calidation approach uses cross validation while resampling before cross validation, which is a common pitfall leading to data leakage. Also a mistake in section 5.7.4 of: https://d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html. It is not the exact same mistake, but similar. Transformations should be applied separately so doesnt really\n",
    "\n",
    "This article aims to improve on this by providing a more methodologically sound approach whilst providing the intuition and explanation for practictioners that are less familiar with imbalanced classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Check the helper_functions\n",
    "    - move _get_num_samples to init?\n",
    "    - Fix scoring function for results\n",
    "    - Should I use fit/transform as opposed to fit_resample/resample in the custom samplers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Notably, to ensure a proper cross validation procedure, I use a pipeline. Specifically, the pipeline from imb_learn is used over sklearn's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt     \n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import CTGANSynthesizer, TVAESynthesizer\n",
    "\n",
    "from helper_functions import NoiseSampler, ColumnScaler, SDVSampler, display_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "For this analysis, the creditcard dataset will be used from Kaggle (https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud), containing transactions and whether they were fraudulent or not. The goal is to predict whether a transaction is fraudulent or not, making it a classification task. Naturally, the amount of non-fraudulent transactions outweigh the number of fraudulent transactions resulting in an imbalanced classification task.\n",
    "\n",
    "For this analysis, we will only be using a subset of the columns, which in this case contains mostly the Principal Components of the original data for confidentiality reasons. In addition, we also have a variable called \"Time\" which is the seconds from the first transaction in the dataset, 'Amount' which is the amount spent on the transaction, and 'Class' indicating whether the transactions is fraudulent or not, which also is our target variable. Ultimately, the data consists mostly of floats with the exception of our target which is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Select first then and last 2 columns\n",
    "creditcard = pd.read_csv('../data/creditcard.csv')\n",
    "\n",
    "# To reduce the dimensiolality of the dataset, we will only use the first 12 principal components,\n",
    "# Time, Amount, and target\n",
    "print(creditcard.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we see how our target labels are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    99.83\n",
       "1     0.17\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target label distribution rounded to 2 decimal places\n",
    "round(creditcard['Class'].value_counts(normalize = True) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes evident that the dataset is highly imbalanced, with  only 0.17% of transactions being fraudulent. This can be problematic for machine learning algorithms if you were to not account for this as models tend to bias towards the non-fraudulent cases given their overrepresentation in the data. As a result, a model predicting every transaction to be non-fraudulent would already result in a 99% accuracy even though every fraudulent transaction has been wrongfully predicted. This highlights the importance of carefully selecting evaluation metrics, as accuracy alone can be misleading in imbalanced classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "Next step is to split the data. We stratify on the target variable to ensure an even split across train/set sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = creditcard.drop('Class', axis = 1)\n",
    "y = creditcard['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "As mentioned previously, the choice of metrics is far from trivial. However, given that it is beyond the scope of this blog, I will choose an F1-score without diving too deep into the costs of misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "For the algorithm, I will use a LGBM Classifier. Choosing the most optimal estimator is beyond the scope of this blog. LGBM is chosen for its efficiency and relative predictive power, therefore being used consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "We define the folds and parameters to optimize over as these will be consistent across resampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2\n",
    "\n",
    "CV_FOLDS = StratifiedKFold(n_splits = 5, random_state = RANDOM_STATE, shuffle = True)\n",
    "\n",
    "scorings = ['f1', 'roc_auc', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrect cross validation procedure\n",
    "\n",
    "Suppose we wish to use ROS to oversample the fraud instances. A common mistake that is made is to perform this oversampling before splitting the data. Why is this wrong: \"Animation here\". To illustrate this we will test two predictive models on truly unseen data. Therefore, for this section we will make use of a train test split. The train set is used to determine the cross validation scores. Next, the cross validation scores are compared. Next predictions are made on the test set and then the predictions on the test set are compared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation mistake is easily made and looks like this implemented in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X, y, test_size = 0.2, stratify = y,\n",
    "                                                    random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# We resample the data used for cross-validation\n",
    "ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "X_res, y_res = ros.fit_resample(X_cv, y_cv)\n",
    "\n",
    "cv_score = cross_validate(\n",
    "    lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1),\n",
    "    X_res, y_res, cv = CV_FOLDS, scoring = scorings, n_jobs = -1\n",
    ")\n",
    "\n",
    "cv_score = cv_score['test_f1'].mean()\n",
    "print(f\"Cross validation score: {cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach leads to (near) perfect scores. Hopefully, this raises some suspicison as to the validity of the results. This seems great, but how do these results translate to truly unseen data: the test set we have not used in the RandomOverSampler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.8442\n"
     ]
    }
   ],
   "source": [
    "# We define the model and fit it to the resampled cv data\n",
    "lgbm_classifier = lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, \n",
    "                                     verbose = -1)\n",
    "lgbm_classifier.fit(X_res, y_res)\n",
    "\n",
    "# predictions on the unseen test data and evaluation\n",
    "preds = lgbm_classifier.predict(X_test)\n",
    "test_score = f1_score(y_test, preds)\n",
    "print(f\"Test score: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test results are lower than the cross validation results indicating that the cross validation score in this way is not representative. Typically you also want to tune hyperparameters during cross validation using GridSearch for example. In this case, this might lead to even worse results because hyperparameters are chosen during incorrect cross validation procedure and fit to that which might results in worse generalization.\n",
    "\n",
    "Although apparent in this situation given the near perfect CV scores, mostly as a results of the severe class imbalance (many observations are duplicates), this won't always be the case with every dataset. In addition, there are other ways of data leakage that are more subtle (explain feature engineering ( including male/female --> 1/0)). Best practice to do everything for train/test separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random OverSampling the Correct Way\n",
    "\n",
    "Next up, we perform cross validation the correct way. There are multiple ways to do it. Namely, iterating over each fold in a loop and manually applying the required transformations to the train and holdout set within each fold separately or using a pipeline. For simplicity sake, we will be using the latter option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score with pipeline: 0.8384\n"
     ]
    }
   ],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_ros = Pipeline(\n",
    "    [('ros', RandomOverSampler(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "# Now we apply the cross-validation on the pipeline using the non resampled data\n",
    "# Subsequently the pipeline will resample the data for each fold separately\n",
    "cv_score_ros = cross_validate(pipeline_ros, X_cv, y_cv, cv = CV_FOLDS, scoring = scorings, \n",
    "                               n_jobs = -1)\n",
    "\n",
    "cv_score_pipe = cv_score_ros['test_f1'].mean()\n",
    "print(f\"Cross validation score with pipeline: {cv_score_pipe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the score is less impressive than the first method, it more accurately reflects the results on the holdout set of 0.8442. Furthermore, (I forgot what I wanted to say)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does this happen? What exactly is going wrong?\n",
    "\n",
    "Compare and display duplicates. I can display duplicates through manual cross validation procedure (before and after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance\n",
    "\n",
    "As a baseline for this problem to compare predictions to, we will use the DummyClassifier from sklearn. We use 'stratified' becasue choosing 'most_frequent' is probably also a natural choice, but given that we mostly evaluate on F1, all scores would be zero, making the improvement over the baseline seem better more impressive.\n",
    "\n",
    "#### Perhaps move this section to later? after the explanation of what goes wrong for instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_baseline = Pipeline(\n",
    "    [('dummy', DummyClassifier(strategy = 'stratified', random_state=RANDOM_STATE)),]\n",
    ")\n",
    "\n",
    "cv_score_base = cross_validate(pipeline_baseline, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                                n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random OverSampling\n",
    "\n",
    "Now on the entire dataset during cross validation instead of a split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_ros = Pipeline(\n",
    "    [('ros', RandomOverSampler(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_ros = cross_validate(pipeline_ros, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise injection\n",
    "\n",
    "The first approach mentioned in the article is noise injection. Although I have not seen it being used in practice and the article does not mention the noise generating process, a uniform sampling procedure will be used. Specifically, for each variable I will extract their minimum and maximum values and use them to sample from a uniform distribution. Therefore, the correlations between variables are overlooked and the bivariate distributions won't be correct.\n",
    "\n",
    "Given that all variables, with exception of the target variable, are numerical this step is quite straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_noise = Pipeline(\n",
    "    [('noise_sampler', NoiseSampler(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_noise = cross_validate(pipeline_noise, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_smote = Pipeline(\n",
    "    [('scaler', ColumnScaler(['Amount', 'Time'])),\n",
    "     ('smote', SMOTE(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_smote = cross_validate(pipeline_smote, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                                n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDV Generators\n",
    "\n",
    "The SDV generators that will be compared are the CTGAN and TVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = Metadata.detect_from_dataframe(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTGAN\n",
    "\n",
    "I will use the CTGAN synthesizer in this case with default parameters and without any constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_ctgan = Pipeline(\n",
    "    [('ctgan', SDVSampler(CTGANSynthesizer, metadata, random_state= RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_ctgan = cross_validate(pipeline_ctgan, X, y, cv = CV_FOLDS, scoring = scorings, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TVAE Synthesizer\n",
    "\n",
    "I will also use the TVAE Synthesizer. Again, there will be no hyperparameter tuning and no constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_tvae = Pipeline(\n",
    "    [('tvae', SDVSampler(TVAESynthesizer, metadata, random_state= RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_tvae = cross_validate(pipeline_tvae, X, y, cv=CV_FOLDS, scoring = scorings,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost sensitive learning\n",
    "\n",
    "I will be using the inverse class frequency for this as the real costs associated with misclassifying creditcard fraud is not known in this example. These weights will be specified using the classifier's 'class_weight' parameter. Specifically, this is set to 'balanced' to achieve the inversely proportional weights. \n",
    "\n",
    "Given that the folds are stratified, we assume the that the assigned weights for the entire train set is (roughly) equal to the weights for each fold. This will result in the following weights being assigned:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.50086524, 289.43800813])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0] / (2 * np.bincount(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline then becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_csl = Pipeline(\n",
    "    [('LGB', lgb.LGBMClassifier(class_weight= 'balanced', random_state = RANDOM_STATE,\n",
    "                                n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_csl = cross_validate(pipeline_csl, X, y, cv=CV_FOLDS, scoring = scorings,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8388369280184002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([3.92413664, 3.97503018, 4.17920709, 3.91475725, 4.23648381]),\n",
       " 'score_time': array([0.27906108, 0.24668121, 0.18132877, 0.25999761, 0.16204786]),\n",
       " 'test_f1': array([0.86458333, 0.83333333, 0.83248731, 0.83838384, 0.82539683]),\n",
       " 'test_roc_auc': array([0.98446967, 0.97377153, 0.9660105 , 0.9700833 , 0.98033602]),\n",
       " 'test_precision': array([0.89247312, 0.80952381, 0.82828283, 0.83      , 0.85714286]),\n",
       " 'test_recall': array([0.83838384, 0.85858586, 0.83673469, 0.84693878, 0.79591837])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cv_score_csl['test_f1'].mean())\n",
    "cv_score_csl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f1', 'roc_auc', 'precision', 'recall']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([8.1566565 , 8.48002768, 8.28060746, 8.15264034, 8.4801054 ]),\n",
       " 'score_time': array([0.25648475, 0.18453979, 0.19741416, 0.2393806 , 0.17600703]),\n",
       " 'test_f1': array([0.86458333, 0.84102564, 0.83597884, 0.81818182, 0.82608696]),\n",
       " 'test_roc_auc': array([0.98166895, 0.97387678, 0.9725407 , 0.96814955, 0.97856439]),\n",
       " 'test_precision': array([0.89247312, 0.85416667, 0.86813187, 0.81      , 0.88372093]),\n",
       " 'test_recall': array([0.83838384, 0.82828283, 0.80612245, 0.82653061, 0.7755102 ])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score_ros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = [cv_score_base, cv_score_noise, cv_score_ros, cv_score_smote,\n",
    "             cv_score_ctgan, cv_score_tvae, cv_score_csl]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noise</th>\n",
       "      <td>0.744</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROS</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE</th>\n",
       "      <td>0.654</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTGAN</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TVAE</th>\n",
       "      <td>0.757</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSL</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1  roc_auc  precision  recall\n",
       "Baseline  0.000    0.499      0.000   0.000\n",
       "Noise     0.744    0.974      0.683   0.823\n",
       "ROS       0.837    0.975      0.862   0.815\n",
       "SMOTE     0.654    0.961      0.538   0.837\n",
       "CTGAN     0.760    0.975      0.718   0.821\n",
       "TVAE      0.757    0.972      0.697   0.831\n",
       "CSL       0.839    0.975      0.843   0.835"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the scores of \n",
    "\n",
    "r = display_scores(cv_scores, scorings)\n",
    "r"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "y_preds = [y_pred_n, y_pred_ros, y_pred_smote, y_pred_ctgan, y_pred_csl]\n",
    "sampling_procedures = ['Noise', 'ROS', 'SMOTE', 'CTGAN', 'Cost-Sensitive Learning']\t\n",
    "\n",
    "get_scores(y_test, y_preds, sampling_procedures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imb_syn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
