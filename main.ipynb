{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data, Imbalanced Labels, and Cross-Validation: Should you use it and how to Prevent Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having worked with synthetic data, I’ve come across many articles that promote tabular synthetic data as a catch-all solution for various obstacles (e.g. machine learning applications, privacy, and testing). While synthetic data definitely is useful, its usefulness and relevance is not always clearly assessed. An example of this is, and also the inspiration for writing this article, is the article provided by Synthetic Data Vault (SDV) titled: \"Can you use synthetic data for label balancing?\" [2].\n",
    "\n",
    "SDV’s article addresses a common challenge in classification: imbalanced target labels. Synthetic data is proposed as a 'compelling solution' for this problem compared to more traditional approaches without any empirical evidence.  Although you definitely can use synthetic data for label balancing (to answer the question of the article), the key question is whether you **should** use synthetic data and how it compares to state-of-the-art techniques.\n",
    "\n",
    "This article addresses the question of the article by SDV on whether you should use synthetic data for imbalanced classification tasks. This article also demonstrates how to set up a proper cross-validation pipeline to prevent data leakage between training and holdout sets when applying resampling techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "We define our random seed for reproducability, define the metrics we will use to evaluate our models, and define the cross-validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "RANDOM_STATE = 2\n",
    "\n",
    "SCORINGS = ['f1', 'precision', 'recall']\n",
    "\n",
    "CV_FOLDS = StratifiedKFold(n_splits = 5, random_state = RANDOM_STATE, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "For this analysis, the creditcard dataset will be used from Kaggle (https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud), containing transactions and whether they were fraudulent or not. To preserve privacy, most of the features are principal components derived from the original dataset. The goal is to predict whether a transaction is fraudulent or not, making it a classification task. Here the 'Class' variable indicates whether the transactions is fraudulent or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "creditcard = pd.read_csv('./data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we see how our target labels are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    99.83\n",
       "1     0.17\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target distribution rounded to 2 decimal places\n",
    "round(creditcard['Class'].value_counts(normalize = True) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the amount of non-fraudulent transactions outweigh the number of fraudulent transactions resulting in an imbalanced classification task. In this situation, the dataset is highly imbalanced, with  only 0.17% of transactions being fraudulent. This imbalance can pose challenges for machine learning models, which may become biased toward predicting the majority class. To address this issue, resampling methods are popular. These are methods to balance the dataset by increasing the minority class or by decreasing the majority class. And sometimes a combination of both, also called hybrid sampling. One of the techniques used for oversampling, is through synthetic data as SDV's article suggests. But how does this approach perform and more importantly compare to other techniques?\n",
    "\n",
    "As you might be thinking (and I was too), this idea probably isn't very novel, and indeed similar research exists in the literature. This led me to the work of Adiputra and Wanchai (2024), which compares similar data-level approaches. However after snooping around in the repository containing the code for their research, I noticed in the validation synthetic data is generated **before** performing cross-validation, which is a common pitfall leading to data leakage and biased results. We'll demonstrate later on how this mistake is easily made and why this leads to data leakage.\n",
    "\n",
    "Ultimately, we'll demonstrate how this data leakage mistake is easily made and why this leads to data leakage. After showing how to prevent this issue, we'll compare synthetic data to other state-of-the-art techniques for imbalanced datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "Next step is to split the data. We split the data into a CV set and a test set. We stratify on the target variable to ensure an even split across sets. The CV set will be used to perform cross validation on and the test will be the untouched data to showcase the effect of improper cross validation procedure and how this generalizes to truely unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = creditcard.drop('Class', axis = 1)\n",
    "y = creditcard['Class']\n",
    "\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X, y, test_size = 0.2, stratify = y,\n",
    "                                                    random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrect cross validation procedure \n",
    "\n",
    "Firstly, displaying what an incorrect cross validation setup looks like. Suppose we wish to use Random OverSampler, which balances the data by duplicating minority class instances, to randomly oversample the fraudulent transactions. A common mistake that is made, also made by Adiputra and Wanchai (2024), which inspired this blog, is to perform this resampling before splitting the data. \n",
    "\n",
    "This cross validation mistake is easily made and looks like this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_validate\n",
    "import lightgbm as lgb\n",
    "\n",
    "# We resample the data before performing cross-validation\n",
    "ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "X_res, y_res = ros.fit_resample(X_cv, y_cv)\n",
    "\n",
    "# Feed the already resampled data into cross validation\n",
    "cv_score = cross_validate(\n",
    "    lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1),\n",
    "    X_res, y_res, cv = CV_FOLDS, scoring = SCORINGS, n_jobs = -1\n",
    ")\n",
    "\n",
    "# Calculate the mean f1 score\n",
    "cv_score = cv_score['test_f1'].mean()\n",
    "print(f\"Cross validation score: {cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach leads to near perfect scores and this seems great, but how do these results translate to truly unseen data: the test set we have not used in the RandomOverSampler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.8442\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# We define the model and fit it to the resampled cv data\n",
    "lgbm_classifier = lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, \n",
    "                                     verbose = -1)\n",
    "lgbm_classifier.fit(X_res, y_res)\n",
    "\n",
    "# predictions on the unseen data\n",
    "preds = lgbm_classifier.predict(X_test)\n",
    "test_score = f1_score(y_test, preds)\n",
    "print(f\"Test score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results are noticeably lower than the cross-validation results, indicating that the cross-validation score obtained this way is not representative of the performance on truly unseen data. Furthermore, you might also want to tune hyperparameters during cross-validation. In this case, hyperparameters are selected based on an incorrect cross-validation procedure and overfit in that process, damaging generalization of your models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is going wrong?\n",
    "\n",
    "Although the difference may seem small, resampling before cross-validation can significantly affect the generalizability of results. The main idea behind creating a train and holdout split is to evaluate the model on data that it has not seen before. Cross-validation applies this same exact concept across multiple folds of the data to limit the variance of the results that come from making one single random split. In each fold, the holdout set is used as a separate unseen set of data to test the model on. Therefore, for each fold in the cross-validation procedure, no information from the holdout set should leak into the train set to ensure that it is still unseen.\n",
    "\n",
    "By using ROS before cross validation in this example, we create duplicates of fraud instances across the ENTIRE dataset. During cross-validation, duplicated fraud instances can appear in the holdout set of a fold whereas the original ends up in the train set of a fold (or vice versa), as shown in Figure 1 below. As a result, the holdout set does not consist of truly unseen data and the model is tasked to predict the outcome of observations it has already seen during training. \n",
    "\n",
    "![Incorrect CV approach](attachments/incorrect_cv.png)\n",
    "\n",
    "Figure 1: Incorrect cross-validation procedure by resampling beforehand for each fold <sub>i<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating this in our example, we look at the exact duplicates of fraudulent transactions across folds when ROS is used before cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates amongst fraudulent transactions in the entire dataset: 0.0650\n"
     ]
    }
   ],
   "source": [
    "# Isolate fraudulent transactions\n",
    "X_pos = X[y == 1]\n",
    "\n",
    "# Determine the proportion of duplicates in the positive class\n",
    "prop_dupl = X_pos.duplicated(keep = False).sum() / X_pos.shape[0]\n",
    "print(f\"Duplicates amongst fraudulent transactions in the entire dataset: {prop_dupl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that approx. 6.5% of fraud instances are not unique. Using stratified folds, we would expect to see a similar percentage of duplicates for each fold throughout cross-validation. \n",
    "\n",
    "First, the number of duplicated fraud instances across folds are displayed for the data that is resampled before cross-validation, i.e. the X_res and y_res from earlier. Specifically, we look into how many duplicates there are between train and holdout for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of duplicates in holdout set across folds: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Again, resampling the data before cross-validation\n",
    "ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "X_res, y_res = ros.fit_resample(X_cv, y_cv)\n",
    "\n",
    "# to track the proportion of duplicates\n",
    "dupl_percentages = []\n",
    "\n",
    "# Loop over each fold using the already resampled data\n",
    "for i, (train_index, hd_index) in enumerate(CV_FOLDS.split(X_res, y_res)):\n",
    "    \n",
    "    # Obtain train and holdout sets for the current fold\n",
    "    X_train, X_hd = X_res.iloc[train_index], X_res.iloc[hd_index]\n",
    "    y_train, y_hd = y_res.iloc[train_index], y_res.iloc[hd_index]\n",
    "    \n",
    "    # Isolate the fraud instances\n",
    "    train_pos = X_train[y_train == 1]\n",
    "    hd_pos = X_hd[y_hd == 1]\n",
    "    \n",
    "    # Check the observations in holdout that are duplicates of the training set for fold i\n",
    "    mask = hd_pos.apply(tuple, axis = 1).isin(train_pos.apply(tuple, axis = 1))\n",
    "    duplicates = hd_pos[mask]\n",
    "    \n",
    "    # Determine the proportion of holdout observation that are duplicates\n",
    "    prop_dupl = duplicates.shape[0] / hd_pos.shape[0] * 100\n",
    "    dupl_percentages.append(prop_dupl)\n",
    "    \n",
    "print(f\"Proportion of duplicates in holdout set across folds: {np.mean(dupl_percentages):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When resampling is performed before cross-validation, the average percentage of duplicates among the fraudulent transactions across folds reaches 100%. This means that every fraud instance in the holdout set is a duplicate of an observation in the training set, explaining the near perfect F1-scores observed earlier.\n",
    "\n",
    "While the high duplication rate is largely attributed to the severe class imbalance in this dataset, the issue may manifest more subtly in other contexts. Moreover, data leakage will not always appear as exact duplicates for all synthesizers. For example, a synthesizer might avoid creating exact duplicates but still generate numerous highly similar observations. Therefore, one should also understand the behavior of the synthesizer used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well... how do you resample the correct way?\n",
    "\n",
    "To resample correctly, we iterate over each fold in a loop, applying the transformations to the training and holdout sets separately. The correct approach is to first split the unresampled data into folds and then apply resampling within each fold, rather than beforehand, as shown in Figure 2.\n",
    "\n",
    "![Correct CV approach](attachments/correct_cv.png)\n",
    "\n",
    "Figure 2: Correct cross-validation procedure by resampling within each fold\n",
    "\n",
    "We begin with a manual implementation of the correct approach to illustrate how the transformations should look and to highlight what went wrong in the previous setup. Afterwards, we will use a pipeline for a more efficient implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with correct CV procedure: 0.8384\n"
     ]
    }
   ],
   "source": [
    "cv_results = []\n",
    "\n",
    "# Loop over the folds of the data that is NOT resampled beforehand\n",
    "for i, (train_index, hd_index) in enumerate(CV_FOLDS.split(X_cv, y_cv)):\n",
    "    \n",
    "    # Obtain train and holdout sets for fold i\n",
    "    X_train, X_hd = X_cv.iloc[train_index], X_cv.iloc[hd_index]\n",
    "    y_train, y_hd = y_cv.iloc[train_index], y_cv.iloc[hd_index]\n",
    "    \n",
    "    # Applying the ROS to the training set WITHIN each fold as opposed to beforehand \n",
    "    ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "    X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Fit the model to the resampled train set and predict the holdout set that is not resampled\n",
    "    lgb_clf = lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1)\n",
    "    lgb_clf.fit(X_train_res, y_train_res)\n",
    "    cv_preds = lgb_clf.predict(X_hd)\n",
    "    \n",
    "    # Calculate and append F1 score for the current fold\n",
    "    cv_score = f1_score(y_hd, cv_preds)\n",
    "    cv_results.append(cv_score)\n",
    "    \n",
    "print(f\"F1 score with correct CV procedure: {np.mean(cv_results):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the score is less impressive than the first method, it more accurately reflects the results on the truly unseen holdout set of 0.8442. And how is this reflected in the duplicates in the holdout set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of duplicates in test set across folds: 6.35%\n"
     ]
    }
   ],
   "source": [
    "dupl_percentages = []\n",
    "\n",
    "# Loop over the folds of the data that is NOT resampled beforehand\n",
    "for i, (train_index, hd_index) in enumerate(CV_FOLDS.split(X_cv, y_cv)):\n",
    "   \n",
    "    # Obtain train and holdout sets for fold i\n",
    "    X_train, X_hd = X_cv.iloc[train_index], X_cv.iloc[hd_index]\n",
    "    y_train, y_hd = y_cv.iloc[train_index], y_cv.iloc[hd_index]\n",
    "    \n",
    "    # WITHIN each fold, we resample the training set\n",
    "    ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "    X_res, y_res = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Only then do we isolate the positive class\n",
    "    train_pos = X_res[y_res == 1]\n",
    "    hd_pos = X_hd[y_hd == 1]\n",
    "    \n",
    "    # Check to see the observations in holdout that are duplicates of the training set\n",
    "    mask = hd_pos.apply(tuple, axis = 1).isin(train_pos.apply(tuple, axis = 1))\n",
    "    duplicates = hd_pos[mask]\n",
    "    \n",
    "    # determine the proportion of holdout observation that are duplicates\n",
    "    prop_dupl = duplicates.shape[0] / hd_pos.shape[0] * 100\n",
    "    dupl_percentages.append(prop_dupl)\n",
    "    \n",
    "print(f\"Proportion of duplicates in test set across folds: {np.mean(dupl_percentages):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can see that the proportion of duplicates across folds resembles the one we obtained from the entire dataset, indicating that resampling within each fold leads to folds that are more representative. Furthermore, the effect of improper cross validation procedure in this example with ROS is clear in this situation due to the large amount of exact duplicates. However, depending on the severity of class imbalance, the synthetic data generator used, and the degree to which the generator may overfit, this data leakage issue might be less obvious. \n",
    "\n",
    "Moreover, this issue of data leakage between train and holdout is not something that only pertains to resampling methods in imbalanced classification tasks. This also applies to other, perhaps more subtle, forms of data leakage, such as feature engineering methods that use information from observations that will end up in the holdout set during cross validation. Think of scaling for instance. Ideally, a scaler should be fit only on the training set and then applied to both training and holdout sets, repeating this for each fold in cross-validation. Not all feature engineering has to lead to data leakage between train and holdout. For instance, the encoding of a categorical gender variable from male/female to binary, which typically is fine since the information is contained within the rows. However, this might get a bit tricky when working with a high cardinality categorical variable where certain values are quite rare and might appear in the hold out set but not in the train set. \n",
    "\n",
    "To avoid these issues, it's important to be aware of what transformations are being applied, when they are applied, and what information they rely on. Best practice is to perform all transformations within cross-validation folds rather than beforehand to prevent any data leakage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to deal with class imbalances?\n",
    "\n",
    "As previously mentioned, imbalanced datasets could lead to biased results towards the larger class if not dealt with. Strategies to handle class imbalance fall into two main categories: data-level and algorithm-level approaches. Data-level approaches modify the dataset by oversampling the minority class, undersampling the majority, or a combination of the two (i.e. hybrid sampling) to balance the dataset. Algorithm-level approaches, in contrast, don't modify the dataset, but instead adjust the algorithm to deal with this imbalance in the dataset.\n",
    "\n",
    "The data-level approaches we use in this article include ROS, noise injection, and synthetic data as covered in SDV's article. While ROS is a common oversampling technique, noise injection is less familiar in practice, and since the article doesn't detail the noise generation process, I assume values are randomly sampled from a normal distribution. Synthetic data refers to artificially generated observations by learning the underlying data patterns. In this scenario, the synthetic data should ideally resemble real observations to help improve model performance. This resemblance is called having high fidelity. However, excessively high fidelity risks creating exact copies of the original data, which can result from the generator overfitting to the data.\n",
    "\n",
    "The techniques we use to generate synthetic data are Synthetic Minority Over-sampling TEchnique (SMOTE), Conditional Tabular Generative Adversarial Networks (CTGAN), and Tabular Variational Auto Encoder (TVAE). We will use SDV's library for the latter two. SMOTE generates synthetic examples of the minority class by interpolating between observations based on their feature space distance, whereas CTGAN and TVAE use neural networks to learn the underlying data distribution and generate new samples without relying on explicit distance-based interpolation.\n",
    "\n",
    "Furthermore, we also use an algorithm-level approach: cost-sensitive learning (CSL). CSL assigns different weights to classes so that the model penalizes misclassification errors differently based on their relative importance. In this context, fraud instances receive higher weights, meaning the model faces a larger penalty for misclassifying fraudulent transactions.\n",
    "\n",
    "CSL is particularly effective when the true costs of different types of misclassification are known. Since these exact costs are unknown, we use inverse class frequency as the weighting scheme, which naturally assigns higher weights to the rarer fraud class. Given that our folds are stratified, we assume that the weights for each fold are (roughy) equal to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class weight: 0.5009, minority class weight: 289.4380\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = X.shape[0] / (2 * np.bincount(y)) \n",
    "print(f\"Majority class weight: {w1:.4f}, minority class weight: {w2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, inspired by Adiputra and Wanchai (2024), we also use hybrid sampling. Specifically, we will throw SMOTE-ENN and their proposed CTGAN-ENN into the mix, which combine oversampling with downsampling using Edited Nearest Neighbors, which removes observations from the majority class that are close to the decision boundary. CTGAN-ENN has been shown to provide improved performance over CSL and other over- and hybrid sampling techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline setup\n",
    "\n",
    "Previously, we have performed cross validation by manually looping over the folds. Although this provides a lot of control, there is an easier and cleaner way to do it. We can use a Pipeline to perform the same operations. A Pipeline offers a more concise and easy to use alternative. The pipelines used will ensure the separation between train and holdout when performing the preprocessing steps for each fold, reducing the risk of data leakage.\n",
    "\n",
    "In this article, we use the Pipeline from the imblearn package. While scikit-learn’s Pipeline can also handle transformations between training and test sets, imblearn’s version supports samplers, making it more suitable for our workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random OverSampling pipeline \n",
    "\n",
    "\n",
    "First, we define the classifier to be used in the pipeline (and in other pipelines). In this article, we arbitrarily choose the LightGBM classifier, but you could replace this with any classifier of your choosing. Next, we construct the pipeline, which includes the preprocessing steps (e.g., our RandomOverSampler, which can be directly passed into the pipeline) followed by the classifier. Finally, we pass this pipeline into the cross_validate function, which subsequently performs all steps separately within each fold, and evaluates the model’s performance across folds using the metrics defined in our SCORINGS list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Define the LightGBM classifier to be used in the pipeline\n",
    "lgb_clf = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Define the sampling pipeline\n",
    "pipeline_ros = Pipeline(\n",
    "    [('ros', RandomOverSampler(random_state = RANDOM_STATE)),\n",
    "     ('lgbm', lgb_clf)] \n",
    ")\n",
    "\n",
    "cv_score_ros = cross_validate(pipeline_ros, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise injection\n",
    "\n",
    "**not shown in blog, only result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import NoiseSampler\n",
    "\n",
    "pipeline_noise = Pipeline(\n",
    "    [('noise_sampler', NoiseSampler(random_state = RANDOM_STATE)),\n",
    "     ('lgbm', lgb_clf)]\n",
    ")\n",
    "\n",
    "cv_score_noise = cross_validate(pipeline_noise, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "**not shown in blog, only result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pipeline_smote = Pipeline(\n",
    "    [('scaler', StandardScaler()),\n",
    "     ('smote', SMOTE(random_state = RANDOM_STATE)),\n",
    "     ('lgbm', lgb_clf)]\n",
    ")\n",
    "\n",
    "cv_score_smote = cross_validate(pipeline_smote, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDV Generators\n",
    "\n",
    "In order to use SDV's synthetic data generators, we first need to define the metadata of the dataset used. This can simply be done with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import Metadata\n",
    "\n",
    "# We first define the metadata for the SDV synthesizers\n",
    "metadata = Metadata.detect_from_dataframe(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to update the metadata manually if you wish. Furthermore, SDV offers a wide variety of synthetic data generators, as well as many functions and parameters to try to improve the quality of the synthetic data. Namely, SDV provides useful preprocessing tools and constraints (i.e., deterministic rules the synthetic data should adhere to) to incorporate domain knowledge and business rules. Since the other methods used in this article work out of the box, I will not be applying any of these methods and will compare them in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTGAN Pipeline \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating CTGAN into a pipeline is a bit more involved since there is no standard support for it like with the RandomOverSampler. Therefore, we have to build our custom resampling class, which we can pass into the pipeline.\n",
    "\n",
    "We first build a BaseSampler class to support resampling procedures that are not natively integrated into the Pipeline from imbalanced-learn (e.g., CTGAN, TVAE, and our custom noise-based sampler). This class provides the shared functionality required across these samplers and ensures that during cross-validation, resampling is applied within each fold while keeping the training and holdout sets properly separated.\n",
    "\n",
    "The class defines a resample() method that performs the actual resampling of the data, and a fit_resample() method that conforms to the scikit-learn/imblearn API by applying resampling only on the training set. In addition, the _get_num_samples() method determines how many synthetic samples of the minority class are required in order to balance the dataset against the majority class in binary classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class BaseSampler(BaseEstimator):\n",
    "    \"\"\"Base class for all custom samplers.\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int) -> None:\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit_resample(self, X: pd.DataFrame, y: pd.Series) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Fit the sampler to the data and return the resampled data\"\"\"\n",
    "        return self.resample(X, y)\n",
    "    \n",
    "    def _get_num_samples(self, y: pd.Series) -> int:\n",
    "        \"\"\"Get the number of samples to generate for the minority class for this binary\n",
    "        classification problem.\"\"\"\n",
    "        y_values = y.value_counts()\n",
    "        num_samples = y_values[0] - y_values[1]\n",
    "        return num_samples  \n",
    "\n",
    "    def resample(self, X: pd.DataFrame, y: pd.Series) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Resample the data using the specified sampling method\"\"\"\n",
    "        \n",
    "        # Create copies of the input data\n",
    "        X_train = X.copy()\n",
    "        y_train = y.copy()\n",
    "        \n",
    "        # Determine the number of samples to generate\n",
    "        num_samples = self._get_num_samples(y_train)\n",
    "\n",
    "        # Isolate the minority class and generate synthetic data\n",
    "        X_minority = X_train[y_train == 1]\n",
    "        X_upsampled = self._generate_syn_data(X_minority, num_samples)\n",
    "\n",
    "        # Updating indepent variables and the target variable\n",
    "        X_sampled = pd.concat([X_train, X_upsampled], axis = 0)\n",
    "        y_sampled = pd.concat([y_train, pd.Series(np.ones(num_samples))], axis = 0)\n",
    "\n",
    "        return X_sampled, y_sampled\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the SDVSampler class for the CTGAN and TVAE synthesizers, based on BaseSampler as its parent class. This class takes the synthesizer generator in the __init__ method and has a custom _generate_syn_data() function to resample using the SDV synthesizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDVSampler(BaseSampler):\n",
    "    \"\"\"Custom samler for SDV synthesizers.\"\"\"\n",
    "    \n",
    "    def __init__(self, generator, metadata: Metadata, random_state: int) -> None:\n",
    "        super().__init__(random_state)\n",
    "        self.generator = generator\n",
    "        self.metadata = metadata\n",
    "       \n",
    "    def _generate_syn_data(self, X_minority: pd.DataFrame, num_samples: int) -> pd.DataFrame:\n",
    "        \"\"\"Generate synthetic data using the SDV synthesizer.\"\"\"\n",
    "        \n",
    "        # Creating a copy of the minority class data\n",
    "        X_resample = X_minority.copy()\n",
    "        \n",
    "        # Creating a new instance of the synthesizer within each fold\n",
    "        synthesizer = self.generator(self.metadata)\n",
    "\n",
    "        # Fitting the synthesizer to the minority class and generating new observations\n",
    "        synthesizer.fit(X_resample)\n",
    "        X_sds = synthesizer.sample(num_samples)\n",
    "        \n",
    "        return X_sds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final pipeline for CTGAN then looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "# Define the CTGAN pipeline using our custom SDVSampler class\n",
    "pipeline_ctgan = Pipeline(\n",
    "    [('ctgan', SDVSampler(CTGANSynthesizer, metadata, random_state = RANDOM_STATE)),\n",
    "     ('lgbm', lgb_clf)]\n",
    ")\n",
    "\n",
    "cv_score_ctgan = cross_validate(pipeline_ctgan, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVAE Synthesizer\n",
    "\n",
    "**not shown in blog, only result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import TVAESynthesizer\n",
    "\n",
    "pipeline_tvae = Pipeline(\n",
    "    [('tvae', SDVSampler(TVAESynthesizer, metadata, random_state = RANDOM_STATE)),\n",
    "     ('lgbm', lgb_clf)]\n",
    ")\n",
    "\n",
    "cv_score_tvae = cross_validate(pipeline_tvae, X, y, cv=CV_FOLDS, scoring = SCORINGS,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE ENN\n",
    "\n",
    "**not shown in blog, only result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "pipeline_smoteenn = Pipeline(\n",
    "    [('scaler', StandardScaler()),\n",
    "     ('smote enn', SMOTEENN(random_state = RANDOM_STATE)),\n",
    "     ('lgbm', lgb_clf)]\n",
    ")\n",
    "\n",
    "cv_score_smoteenn = cross_validate(pipeline_smoteenn, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                                n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTGAN ENN\n",
    "\n",
    "**not shown in blog, only result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import SDVENN\n",
    "\n",
    "pipeline_ctganenn = Pipeline(\n",
    "    [('ctgan', SDVENN(CTGANSynthesizer, metadata, random_state = RANDOM_STATE)),\n",
    "     ('lgnm', lgb_clf)]\n",
    ")\n",
    "\n",
    "cv_score_ctganenn = cross_validate(pipeline_ctganenn, X, y, cv = CV_FOLDS, \n",
    "                                   scoring = SCORINGS, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm level approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost sensitive learning\n",
    "\n",
    "**not shown in blog, only result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_csl = Pipeline(\n",
    "    [('LGB', lgb.LGBMClassifier(class_weight= 'balanced', random_state = RANDOM_STATE,\n",
    "                                n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_csl = cross_validate(pipeline_csl, X, y, cv=CV_FOLDS, scoring = SCORINGS,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Noise</th>\n",
       "      <td>0.746</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROS</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE</th>\n",
       "      <td>0.663</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTGAN</th>\n",
       "      <td>0.766</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TVAE</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTENN</th>\n",
       "      <td>0.649</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTGANENN</th>\n",
       "      <td>0.812</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSL</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             F1  Precision  Recall\n",
       "Noise     0.746      0.684   0.825\n",
       "ROS       0.837      0.862   0.815\n",
       "SMOTE     0.663      0.552   0.831\n",
       "CTGAN     0.766      0.747   0.801\n",
       "TVAE      0.775      0.730   0.827\n",
       "SMOTENN   0.649      0.521   0.862\n",
       "CTGANENN  0.812      0.920   0.730\n",
       "CSL       0.839      0.843   0.835"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading a custom function to display the results\n",
    "from helper_functions import display_scores\n",
    "\n",
    "cv_scores = [cv_score_noise, cv_score_ros, cv_score_smote, cv_score_ctgan, cv_score_tvae, \n",
    "             cv_score_smoteenn, cv_score_ctganenn, cv_score_csl]   \n",
    "\n",
    "result = display_scores(cv_scores, SCORINGS)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, none of the data-level approaches using synthetic data rank among the best-performing methods, while techniques using SMOTE actually delivered the worst performance. In terms of F1-score, the algorithm-level approach CSL performs best with a score of 0.839. The top-performing data-level approach is ROS, which closely follows CSL with a slightly lower score of 0.837.\n",
    "\n",
    "Interestingly, the F1-scores for CTGAN and TVAE (0.766 and 0.775, respectively) are closer to the noise imputation (0.741) than to CSL's performance, suggesting that exploring SDV's pre-processors and constraints could be a relevant next step for improving performance, as mentioned previously.\n",
    "\n",
    "SMOTE and SMOTE-ENN provide surprisingly low F1-scores, likely due to the dataset's high dimensionality. While the hybrid sampling approach of SMOTE-ENN does not improve upon SMOTE alone, Adiputra and Wanchai (2024)'s proposed CTGAN-ENN achieved an F1-score of 0.812, outperforming standard CTGAN and even resulting in the highest precision among all evaluated techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we have seen that resampling and other pre-processing steps should not be performed before cross-validation to prevent data leakage. Ideally, these steps should be performed during cross-validation, independently within each fold, which can easily be accomplished using a pipeline.\n",
    "\n",
    "This analysis also highlights that synthetic data is not always the best solution for imbalanced classification tasks. While synthetic data can be applied to such problems, other state-of-the-art techniques may provide better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] I. N. M. Adiputra and P. Wanchai, CTGAN-ENN: a tabular GAN-based hybrid sampling method for imbalanced and overlapped data in customer churn prediction (2024), Journal of Big Data, vol. 11, no. 1, Sep. 2024\n",
    "\n",
    "[2] Neha Patki, Can You Use Synthetic Data for Label Balancing? (2023), DataCebo Blog\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imb_syn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
