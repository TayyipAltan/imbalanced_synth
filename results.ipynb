{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “Synthetic Data for Label Balancing: Should You Use It and How?”\n",
    "\n",
    "AOver the past six months working with tabular synthetic data, I’ve come across many articles that promote it as a catch-all solution for various machine learning challenges. While synthetic data can serve as a useful Privacy Enhancing Technology (PET) and has shown to be useful in certain tasks, its usefulness and relevance is not always clearly assessed. An example of this is, and also the inspiration for me writing this article, is the article provided by Synthetic Data Vault (SDV) titled: \"Can you use synthetic data for label balancing?\" (https://sdv.dev/blog/synthetic-label-balancing/).\n",
    "\n",
    "SDV’s article addresses a common challenge in classification: imbalanced target labels. It discusses traditional data-level solutions such as noise injection and Random Oversampling (ROS), correctly noting their limitations. However, it then proposes synthetic data as a 'compelling solution' without any empirical evidence. While I am a fan of SDV’s generators, constraints, and preprocessors, this particular argument overlooks important aspects of evaluating synthetic data for label balancing. Although you definitely can use synthetic data for label balancing (to answer the question of the article), the key question is whether you **should** use synthetic data and how it compares to state-of-the-art (SOTA) techniques.\n",
    "\n",
    "Throughout this article, I aim to provide an answer to this question by comparing synthetic data produced by SDV generators against alternatives and build on top of the aformentioned article. Specifically, I compare data-level approaches such as noise injection, ROS, and Synthetic Minority Over-sampling TEchnique (SMOTE) to synthetic data generators by SDV such as Conditional Tabular Generative Adversarial Networks (CTGAN), Tabular Variational AutoEncoder (TVAE) and the Gaussian Copula. These are also compared against the algorithm-level approach of Cost-Sensitive learning. \n",
    "\n",
    "As you might be thinking (similar to me), this idea is likely not very novel and indeed similar research is available in literature. Adiputra and Wanchai (2024) for instance compare similar approaches resampling and synthetic data approaches. However, their validation approach uses cross validation (CV) with synthetic data being generated before CV, which is a common pitfall leading to data leakage between train and hold out within each fold leading to biased results. (Also a mistake in section 5.7.4 of: https://d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html. It is not the exact same mistake, but similar. Transformations should be applied separately so doesnt really. Mention this in the conclusion that you should also split other feature engineering methods in CV)\n",
    "\n",
    "This article aims to provide an answer to the question of the article by SDV whether you should use synthetic data for imbalanced classification tasks. Furthermore, this article also aims to address pitfalls in cross validation leading to data leakage between train and holdout fold, why this is problematic, and how you can correctly set up a CV procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Notably, to ensure a proper cross validation procedure, I use a pipeline. Specifically, the pipeline from imb_learn is used over sklearn's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt     \n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import CTGANSynthesizer, TVAESynthesizer, GaussianCopulaSynthesizer\n",
    "\n",
    "from helper_functions import NoiseSampler, ColumnScaler, SDVSampler, display_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "For this analysis, the creditcard dataset will be used from Kaggle (https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud), containing transactions and whether they were fraudulent or not. To preserve privacy, most of the features are principal components derived from the original dataset. The goal is to predict whether a transaction is fraudulent or not, making it a classification task. Here the 'Class' variable indicates whether the transactions is fraudulent or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/creditcard.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# We read in the credit card fraud dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m creditcard = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data/creditcard.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Altan\\.conda\\envs\\slr_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    899\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    900\u001b[39m     dialect,\n\u001b[32m    901\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    908\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    909\u001b[39m )\n\u001b[32m    910\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Altan\\.conda\\envs\\slr_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    574\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    576\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Altan\\.conda\\envs\\slr_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1404\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1406\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1407\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Altan\\.conda\\envs\\slr_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1659\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1660\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1661\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1672\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Altan\\.conda\\envs\\slr_env\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    855\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    856\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    858\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    867\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    868\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './data/creditcard.csv'"
     ]
    }
   ],
   "source": [
    "# We read in the credit card fraud dataset\n",
    "creditcard = pd.read_csv('./data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we see how our target labels are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    99.83\n",
       "1     0.17\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target label distribution rounded to 2 decimal places\n",
    "round(creditcard['Class'].value_counts(normalize = True) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the amount of non-fraudulent transactions outweigh the number of fraudulent transactions resulting in an imbalanced classification task. It is highly imbalanced, with  only 0.17% of transactions being fraudulent. This imbalance can pose challenges for machine learning models, which may become biased toward predicting the majority class. To address this issue, several data-level techniques and algorithm-level techniques exist.\n",
    "\n",
    "Explain data level approaches, and why I write this article\n",
    "\n",
    "Explain algorithm level approaches\n",
    "\n",
    "Explain that this analysis is not new, but there are issues with proper cross validation.\n",
    "- Perhaps I only cover this last point here and re-iterate the approaches as a short recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics & Algorithm\n",
    "\n",
    "Given that the dataset is imbalanced, using accuracy as a metric would not provide a good representation of the performance of the models as blindly predicting everything to be non-fraudulent would already result in a 99% accuracy. Typically choosing the appropriate metric requires considering the missclassification costs within the context. However, this is beyond the scope of this blog, and therefore I will choose an F1-score without diving too deep into this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "For the algorithm, I will use a LGBM Classifier. Choosing the most optimal estimator is beyond the scope of this blog. LGBM is chosen for its efficiency and relative predictive power, therefore being used consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "We define the folds and parameters to optimize over as these will be consistent across resampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2\n",
    "\n",
    "CV_FOLDS = StratifiedKFold(n_splits = 5, random_state = RANDOM_STATE, shuffle = True)\n",
    "\n",
    "SCORINGS = ['f1', 'roc_auc', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "Next step is to split the data. We split the data into a CV set and a test set. We stratify on the target variable to ensure an even split across sets. The CV set will be used to perform cross validation on and the test will be the untouched data to showcase the effect of improper cross validation procedure and how this generalizes to truely unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = creditcard.drop('Class', axis = 1)\n",
    "y = creditcard['Class']\n",
    "\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X, y, test_size = 0.2, stratify = y,\n",
    "                                                    random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrect cross validation procedure\n",
    "\n",
    "Firstly, displaying what an incorrect cross validation setup looks like. Suppose we wish to use ROS to randomly oversample the fraud instances. A common mistake that is made, also by Adiputra and Wanchai (2024), is to perform this oversampling before splitting the data. \n",
    "\n",
    "This cross validation mistake is easily made and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# We resample the data before performing cross-validation\n",
    "ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "X_res, y_res = ros.fit_resample(X_cv, y_cv)\n",
    "\n",
    "# We feed the already resampled data to the cross-validation\n",
    "cv_score = cross_validate(\n",
    "    lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1),\n",
    "    X_res, y_res, cv = CV_FOLDS, scoring = SCORINGS, n_jobs = -1\n",
    ")\n",
    "\n",
    "cv_score = cv_score['test_f1'].mean()\n",
    "print(f\"Cross validation score: {cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach leads to near perfect scores. Hopefully, this raises some suspicison as to the validity of the results. This seems great, but how do these results translate to truly unseen data: the test set we have not used in the RandomOverSampler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.8442\n"
     ]
    }
   ],
   "source": [
    "# We define the model and fit it to the resampled cv data\n",
    "lgbm_classifier = lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, \n",
    "                                     verbose = -1)\n",
    "lgbm_classifier.fit(X_res, y_res)\n",
    "\n",
    "# predictions on the unseen test data and evaluation\n",
    "preds = lgbm_classifier.predict(X_test)\n",
    "test_score = f1_score(y_test, preds)\n",
    "print(f\"Test score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results are lower than the cross-validation results, indicating that the cross-validation score obtained this way is not representative of the performance on truly unseen data. Furthermore, you might also want to tune hyperparameters during cross-validation. In this case, it might lead to even worse results, as hyperparameters are selected based on an incorrect cross-validation procedure and overfit to that process, potentially worsening generalization.\n",
    "\n",
    "Although the data leakage between train and holdout sets within each fold is evident here given the near perfect CV scores, this won't always be the case. This dataset contains a severe class imbalance and ROS is used, leading to many exact duplicates across train and holdout. Depending on the class imbalance, the synthetic data generator, and the degree to which the generator may overfit, this data leakage issue might be less obvious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you use ROS the correct way?\n",
    "\n",
    "There are multiple ways to do it. Namely, iterating over each fold in a loop and manually applying the transformations to the train and holdout set separately or by using a pipeline. A pipeline offers a more concise and easy to use alternative.\n",
    "\n",
    "First, a manual loop is used to get a better feel of how the transformations should look like and what is exactly is going wrong in the previous setup. Afterwards, a pipeline will be used for more efficient coding.   !!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with correct CV procedure: 0.8384\n"
     ]
    }
   ],
   "source": [
    "# track the test scores for each fold and average them afterwards\n",
    "cv_results = []\n",
    "\n",
    "for i, (train_index, hd_index) in enumerate(CV_FOLDS.split(X_cv, y_cv)):\n",
    "    \n",
    "    # Obtain train and holdout sets for the current fold\n",
    "    X_train, X_hd = X_cv.iloc[train_index], X_cv.iloc[hd_index]\n",
    "    y_train, y_hd = y_cv.iloc[train_index], y_cv.iloc[hd_index]\n",
    "    \n",
    "    # Applying the ROS to the training set of the current fold as opposed to beforehand for the entire cv set\n",
    "    ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "    X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Fit the model to train and predict the holdout set that is not resampled\n",
    "    lgb_clf = lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1)\n",
    "    lgb_clf.fit(X_train_res, y_train_res)\n",
    "    cv_preds = lgb_clf.predict(X_hd)\n",
    "    \n",
    "    # Calculate and append F1 score for the current fold\n",
    "    cv_score = f1_score(y_hd, cv_preds)\n",
    "    cv_results.append(cv_score)\n",
    "    \n",
    "print(f\"F1 score with correct CV procedure: {np.mean(cv_results):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the score is less impressive than the first method, it more accurately reflects the results on the truly unseen holdout set of 0.8442. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is going wrong?\n",
    "\n",
    "Although the difference may seem small, resampling before cross-validation can significantly affect the generalizability of results. The main idea behind creating a train and holdout split is to evaluate the model on data that it has not seen before. Cross-validation applies this same exact concept across multiple folds of the data to limit the variance of the results that come from making one single random split. In each fold, the holdout set is used as a separate unseen set of data to test the model on. Therefore, for each fold in the cross-validation procedure, no information from the holdout set should leak into the train set to ensure that it is still unseen. **Add a source here that better explains why we use cross validation**\n",
    "\n",
    "By using ROS before cross validation in this example, we create duplicates of fraud instances across the ENTIRE dataset. During cross-validation, these duplicated fraud instances can end up in the holdout set of a fold whereas the original ends up in the train set of a fold (or vice versa). As a result, the holdout set does not consist of truly unseen data and the model is tasked to predict the outcome of an observation it has already seen during training.\n",
    "\n",
    "To demonstrate this, we look at the exact duplicates across folds when ROS is used before and during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates amongst fraudulent transactions: 0.0650\n"
     ]
    }
   ],
   "source": [
    "# Isolate the positive class of the original unresampled dataset\n",
    "X_pos = X[y == 1]\n",
    "\n",
    "# Determining the proportion of duplicates in the positive class\n",
    "prop_dupl = X_pos.duplicated(keep = False).sum() / X_pos.shape[0]\n",
    "print(f\"Duplicates amongst fraudulent transactions: {prop_dupl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that approx. 6.5% of fraud instances are not unique. Using stratification across the folds, we would expect to see a similar percentage of duplicates throughout cross-validation. \n",
    "\n",
    "First, the number of duplicated fraud instances across folds are displayed for the data that is resampled before cross-validation, i.e. the X_res and y_res from earlier. More specifically, we look into how many duplicates there are between train and holdout for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of duplicates in holdout set across folds: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Again, resampling the data before cross-validation\n",
    "ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "X_res, y_res = ros.fit_resample(X_cv, y_cv)\n",
    "\n",
    "# to track the proportion of duplicates\n",
    "dupl_percentages = []\n",
    "\n",
    "# Loop over each fold using the already resampled data\n",
    "for i, (train_index, hd_index) in enumerate(CV_FOLDS.split(X_res, y_res)):\n",
    "    \n",
    "    # Obtain train and holdout sets for the current fold\n",
    "    X_train, X_hd = X_res.iloc[train_index], X_res.iloc[hd_index]\n",
    "    y_train, y_hd = y_res.iloc[train_index], y_res.iloc[hd_index]\n",
    "    \n",
    "    # Isolate the fraud instances\n",
    "    train_pos = X_train[y_train == 1]\n",
    "    hd_pos = X_hd[y_hd == 1]\n",
    "    \n",
    "    # Check to see the observations in holdout that are duplicates of the training set\n",
    "    mask = hd_pos.apply(tuple, axis = 1).isin(train_pos.apply(tuple, axis = 1))\n",
    "    duplicates = hd_pos[mask]\n",
    "    \n",
    "    # Determine the proportion of holdout observation that are duplicates\n",
    "    prop_dupl = duplicates.shape[0] / hd_pos.shape[0] * 100\n",
    "    dupl_percentages.append(prop_dupl)\n",
    "    \n",
    "print(f\"Proportion of duplicates in holdout set across folds: {np.mean(dupl_percentages):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average percentage of duplicates across folds is 100% if we resample before performing cross-validation. This means that all of the fraud instances in the holdout set are duplicates of observations in the train set, leading to the near perfect F1-score we saw earlier. Although the high percentage of duplicates is largely due to the severe class imbalance, and this issue will not always present itself in the form of exact dupliates for other synthesizers, this approach is still fundamentally flawed. It does not represent a valid evaluation setup, as synthetic or resampled data would not be used as a holdout set for testing a real model. Overall, the information contained in the observations that end up in the holdout set should not be used to transform the observations that end up in the train set, a concept which is often overlooked when applying cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, how many duplicates would we have if we applied resampling correctly in our cross-validation procedure? The same steps as above are repeated, i.e. checking the number of duplicates in the holdout set, but now a valid cross-validation procedure is used by applying resampling WITHIN each fold as opposed to beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of duplicates in test set across folds: 6.35%\n"
     ]
    }
   ],
   "source": [
    "# Again, we track the proportion of duplicates \n",
    "dupl_percentages = []\n",
    "\n",
    "for i, (train_index, hd_index) in enumerate(CV_FOLDS.split(X_cv, y_cv)):\n",
    "   \n",
    "    # Obtain train and holdout sets for the current fold\n",
    "    X_train, X_hd = X_cv.iloc[train_index], X_cv.iloc[hd_index]\n",
    "    y_train, y_hd = y_cv.iloc[train_index], y_cv.iloc[hd_index]\n",
    "    \n",
    "    # Within each fold, we resample the training set\n",
    "    ros = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "    X_res, y_res = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Only then do we isolate the positive class\n",
    "    train_pos = X_res[y_res == 1]\n",
    "    hd_pos = X_hd[y_hd == 1]\n",
    "    \n",
    "    # Check to see the observations in holdout that are duplicates of the training set\n",
    "    mask = hd_pos.apply(tuple, axis = 1).isin(train_pos.apply(tuple, axis = 1))\n",
    "    duplicates = hd_pos[mask]\n",
    "    \n",
    "    # determine the proportion of holdout observation that are duplicates\n",
    "    prop_dupl = duplicates.shape[0] / hd_pos.shape[0] * 100\n",
    "    dupl_percentages.append(prop_dupl)\n",
    "    \n",
    "print(f\"Proportion of duplicates in test set across folds: {np.mean(dupl_percentages):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of improper cross validation procedure in this example with ROS is clear due to the high amount of data leakage and the exact duplicates. However, when using synthetic data generators such as the ones used in SDV, this might be less obvious depending on how much they overfit to the data. This also varies depending on the generator. As some generator tend to produce more direct duplicates of the original training data than others. \n",
    "\n",
    "However, this issue of data leakge between train and test is not something that only pertains to resampling methods in imbalanced classification tasks. This is also valid for other feature engineering methods that use information from observations that will end up in the hold out fold during cross validation. Think of scaling/normalizing, mean imputation etc. which would also lead to data leakage. \n",
    "\n",
    "Though not all feature engineering has to lead to data leakage between train and fold. Think of encoding a categorical gender variable from male/female to binary, which typically is fine since the information is contained within the rows. However, this might get a bit tricky when working with a high cardinality categorical variable where certain values are very rare meaning that they might occur in the hold out fold but not in the train fold. Similarly with word embeddings based on words that end up in the holdout fold but not in the train fold. For this reason, you should always be aware of the transformations you are doing, when you are doing them, and which information is used in those transformations. Therefore, it is best practice to perform all data transformations within the cross validation procedure as opposed to beforehand.\n",
    "\n",
    "\n",
    "---------------------\n",
    "\n",
    "\n",
    "\n",
    "This applies not only to oversampling in imbalanced datasets but also to other, perhaps more subtle, forms of data leakage. Such as feature engineering steps that use information from the entire datasets (including observations that may end up in the holdout set) may leak information into the holdout set. Think of scaling. Ideally, a scaler should be fit only on the training set and then applied to both training and holdout sets, repeating this for each fold in cross-validation. \n",
    "\n",
    "This concept of applying feature engineering steps separately across sets technically isn't always necessary. For instance, transforming a categorical variable, such as male/female, to a binary variable. This is typically fine as the information is contained within the rows. However, this might get complicated if you are dealing with a categorical variable with high cardinality, as the holdout set may contain categories not seen in the training set.\n",
    "\n",
    "\n",
    "This highlights why it is important to think of ways data leakage could occur between sets, that data leakage could occur during cross validation as well and why it is good practice to perform transformation separately on training and holdout sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performances on imbalanced classification tasks\n",
    "\n",
    "As mentioned previously, we compare the performances based on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance\n",
    "\n",
    "As a baseline for this problem to compare predictions to, we will use the DummyClassifier from sklearn. We use 'stratified' becasue choosing 'most_frequent' is probably also a natural choice, but given that we mostly evaluate on F1, all scores would be zero, making the improvement over the baseline seem better more impressive.\n",
    "\n",
    "#### Perhaps move this section to later? after the explanation of what goes wrong for instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_baseline = Pipeline(\n",
    "    [('dummy', DummyClassifier(strategy = 'stratified', \n",
    "                               random_state = RANDOM_STATE))]\n",
    ")\n",
    "\n",
    "cv_score_base = cross_validate(pipeline_baseline, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data level approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random OverSampling\n",
    "\n",
    "Now on the entire dataset during cross validation instead of a split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_ros = Pipeline(\n",
    "    [('ros', RandomOverSampler(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_ros = cross_validate(pipeline_ros, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise injection\n",
    "\n",
    "The first approach mentioned in the article is noise injection. Although I have not seen it being used in practice and the article does not mention the noise generating process, a uniform sampling procedure will be used. Specifically, for each variable I will extract their minimum and maximum values and use them to sample from a uniform distribution. Therefore, the correlations between variables are overlooked and the bivariate distributions won't be correct.\n",
    "\n",
    "Given that all variables, with exception of the target variable, are numerical this step is quite straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_noise = Pipeline(\n",
    "    [('noise_sampler', NoiseSampler(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_noise = cross_validate(pipeline_noise, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_smote = Pipeline(\n",
    "    [('scaler', ColumnScaler(['Amount', 'Time'])),\n",
    "     ('smote', SMOTE(random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_smote = cross_validate(pipeline_smote, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                                n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDV Generators\n",
    "\n",
    "The SDV generators that will be compared are the CTGAN and TVAE. Also use Gaussian Copula?? depending on the distributions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = Metadata.detect_from_dataframe(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Copula\n",
    "\n",
    "Finally, I will use the Gaussian Copula. The Gaussian Copula is a probabilistic model, making it more easily understood than the more complex GANs and VAEs. \n",
    "\n",
    "After a short inspection of the distributions, the gamma distribution is used as a default distribution for all variables in favour of choosing separate best fitting disitrbutions of for each variable specifically and using the kernal density estimation to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_gc = Pipeline(\n",
    "    [('gc', SDVSampler(GaussianCopulaSynthesizer, metadata, random_state = RANDOM_STATE,\n",
    "                       def_distr = 'gamma')),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_gc = cross_validate(pipeline_gc, X, y, cv=CV_FOLDS, scoring = SCORINGS,\n",
    "                             n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTGAN\n",
    "\n",
    "I will use the CTGAN synthesizer in this case with default parameters and without any constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_ctgan = Pipeline(\n",
    "    [('ctgan', SDVSampler(CTGANSynthesizer, metadata, random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_ctgan = cross_validate(pipeline_ctgan, X, y, cv = CV_FOLDS, scoring = SCORINGS, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVAE Synthesizer\n",
    "\n",
    "I will also use the TVAE Synthesizer. Again, there will be no hyperparameter tuning and no constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_tvae = Pipeline(\n",
    "    [('tvae', SDVSampler(TVAESynthesizer, metadata, random_state = RANDOM_STATE)),\n",
    "     ('LGB', lgb.LGBMClassifier(random_state = RANDOM_STATE, n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_tvae = cross_validate(pipeline_tvae, X, y, cv=CV_FOLDS, scoring = SCORINGS,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm level approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost sensitive learning\n",
    "\n",
    "I will be using the inverse class frequency for this as the real costs associated with misclassifying creditcard fraud is not known in this example. These weights will be specified using the classifier's 'class_weight' parameter. Specifically, this is set to 'balanced' to achieve the inversely proportional weights. \n",
    "\n",
    "Given that the folds are stratified, we assume the that the assigned weights for the entire train set is (roughly) equal to the weights for each fold. This will result in the following weights being assigned:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.50086524, 289.43800813])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0] / (2 * np.bincount(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline then becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling pipeline\n",
    "pipeline_csl = Pipeline(\n",
    "    [('LGB', lgb.LGBMClassifier(class_weight= 'balanced', random_state = RANDOM_STATE,\n",
    "                                n_jobs = -1, verbose = -1))]\n",
    ")\n",
    "\n",
    "cv_score_csl = cross_validate(pipeline_csl, X, y, cv=CV_FOLDS, scoring = SCORINGS,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = [cv_score_base, cv_score_noise, cv_score_ros, cv_score_smote,\n",
    "             cv_score_ctgan, cv_score_tvae, cv_score_gc, cv_score_csl]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc_auc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noise</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROS</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE</th>\n",
       "      <td>0.654</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTGAN</th>\n",
       "      <td>0.764</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TVAE</th>\n",
       "      <td>0.763</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Copula</th>\n",
       "      <td>0.847</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSL</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    F1  Roc_auc  Precision  Recall\n",
       "Baseline         0.000    0.499      0.000   0.000\n",
       "Noise            0.708    0.974      0.626   0.819\n",
       "ROS              0.837    0.975      0.862   0.815\n",
       "SMOTE            0.654    0.961      0.538   0.837\n",
       "CTGAN            0.764    0.971      0.721   0.817\n",
       "TVAE             0.763    0.967      0.715   0.821\n",
       "Gaussian Copula  0.847    0.978      0.906   0.797\n",
       "CSL              0.839    0.975      0.843   0.835"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the scores of \n",
    "r = display_scores(cv_scores, SCORINGS)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian copula provides the best F1, which is kind of surprising. This is likely very dataset dependent and to rule out/for synthetic data, this process should in actuality be repeated for multiple datasets, which I don't have the time to do. Perhaps I should add a quick section: 'Understanding the data', since the dataset at hand might be over simplistic.\n",
    "\n",
    "However, the Gaussian Copula provides the worsy recall of all methods (and best precision). For the other methods, this is more aligned with each other. So, it would also still be dependent on how you want to use the model and the cost of misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
